{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33114de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ['XMNLP_MODEL'] = 'xmnlp-onnx-models'\n",
    "\n",
    "# 确认环境变量已设置\n",
    "print(os.environ['XMNLP_MODEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP训练模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 读取数据\n",
    "shurupoi = pd.read_csv('shurupoi.csv')\n",
    "biaozhupoi = pd.read_csv('biaozhupoi.csv')\n",
    "\n",
    "# 确保shurupoi DataFrame中ID的唯一性\n",
    "assert shurupoi['id'].is_unique\n",
    "\n",
    "# 定义距离计算函数\n",
    "def calculate_distance(p1, p2):\n",
    "    return (3000 - geodesic((p1['lat'], p1['lng']), (p2['lat'], p2['lng'])).meters) / 3000\n",
    "\n",
    "# 定义类别相似度计算函数\n",
    "def calculate_category_similarity(p1, p2):\n",
    "    if p1['t'] == p2['t'] and p1['c'] == p2['c']:\n",
    "        return 1\n",
    "    elif p1['t'] == p2['t']:\n",
    "        return 0.7\n",
    "    else:\n",
    "        return 0.3\n",
    "\n",
    "# 定义名称相似度计算函数\n",
    "def calculate_name_similarity(p1, p2):\n",
    "    # 这里需要确保name是字符串类型，如果它们是列表，则应转换为字符串再计算\n",
    "    intersection = len(set(p1['name']) & set(p2['name']))\n",
    "    union = len(set(p1['name']) | set(p2['name']))\n",
    "    fiu = intersection / union if union > 0 else 0\n",
    "    return fiu\n",
    "\n",
    "# 定义评论数量相似度计算函数\n",
    "def calculate_comment_similarity(p1, p2):\n",
    "    return (2 * min(p1['q'], p2['q'])) / (p1['q'] + p2['q']) if p1['q'] + p2['q'] > 0 else 0\n",
    "\n",
    "class PoiDataset(Dataset):\n",
    "    def __init__(self, shurupoi, biaozhupoi):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # 添加排序优先级字段\n",
    "        biaozhupoi['sort_priority'] = biaozhupoi.groupby('id').cumcount()\n",
    "\n",
    "        # 预处理数据\n",
    "        for index, row in biaozhupoi.iterrows():\n",
    "            p1 = shurupoi[shurupoi['id'] == row['id']]\n",
    "            p2 = shurupoi[shurupoi['id'] == row['pid']]\n",
    "            \n",
    "            assert len(p1) == 1, \"There should be exactly one match for ID.\"\n",
    "            assert len(p2) == 1, \"There should be exactly one match for PID.\"\n",
    "            \n",
    "            p1 = p1.iloc[0]\n",
    "            p2 = p2.iloc[0]\n",
    "            \n",
    "            distance = calculate_distance(p1, p2)\n",
    "            category_similarity = calculate_category_similarity(p1, p2)\n",
    "            name_similarity = calculate_name_similarity(p1, p2)\n",
    "            comment_similarity = calculate_comment_similarity(p1, p2)\n",
    "            label = row['ss']\n",
    "            sort_priority = row['sort_priority']  # 新增排序优先级\n",
    "            \n",
    "            # 对相同ss值的样本添加微小的噪声，以反映排序优先级\n",
    "            if sort_priority != 0:\n",
    "                # 例如，添加一个极小的负数，使得排序靠后的PID有略低的相似度\n",
    "                name_similarity += -0.001 * sort_priority\n",
    "                \n",
    "            self.features.append([distance, category_similarity, name_similarity, comment_similarity, sort_priority])\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = PoiDataset(shurupoi, biaozhupoi)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 定义多层感知器模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)  # 修改输出层，使其输出一个连续值\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = MLP()\n",
    "criterion = nn.MSELoss()  # 更换损失函数为MSE\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 评估\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs).view(-1)  # 确保输出形状与标签形状一致\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(outputs.tolist())\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    return mse, mae\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).view(-1)  # 确保输出形状与标签形状一致\n",
    "        loss = criterion(outputs, labels.float())  # 确保标签类型为float\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# 在训练结束后评估模型\n",
    "mse, mae = evaluate(model, dataloader)\n",
    "print(f'Mean Squared Error: {mse:.4f}, Mean Absolute Error: {mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 景点类选择KNN图示\n",
    "# 不去重和近义词\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from word_similarity import WordSimilarity2010\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 定义一个清理名称的函数，移除括号及其内容\n",
    "def clean_name(name):\n",
    "    # 使用正则表达式移除括号及其内容\n",
    "    return re.sub(r'\\(.*\\)', '', name).strip()\n",
    "\n",
    "# 修改后的名称相似度计算函数\n",
    "def calculate_name_similarity(p1, p2):\n",
    "    # 清理两个POI的名称\n",
    "    cleaned_name1 = clean_name(p1['name'])\n",
    "    cleaned_name2 = clean_name(p2['name'])\n",
    "    \n",
    "    # 现在使用清理后的名称进行比较\n",
    "    intersection = len(set(cleaned_name1) & set(cleaned_name2))\n",
    "    union = len(set(cleaned_name1) | set(cleaned_name2))\n",
    "    fiu = intersection / union if union > 0 else 0\n",
    "    return fiu\n",
    "\n",
    "# 其他代码保持不变...\n",
    "# 读取shurupoi1.csv文件\n",
    "shurupoi1 = pd.read_csv('shurupoi5.csv')\n",
    "\n",
    "# 定义获取指定ID的POI与shurupoi1.csv中其他POI的相似度的函数\n",
    "def get_similar_pois_within_shurupoi1(model, poi_id):\n",
    "    # 获取输入poi的信息\n",
    "    input_poi = shurupoi1.loc[shurupoi1['id'] == poi_id].iloc[0]\n",
    "    \n",
    "    # 初始化相似度分数列表\n",
    "    similarity_scores = []\n",
    "    \n",
    "    # 遍历shurupoi1.csv中的所有poi，计算相似度分数\n",
    "    for idx, row in shurupoi1.iterrows():\n",
    "        if row['id'] != poi_id:  # 确保不计算与自身之间的相似度\n",
    "            p2 = row\n",
    "            distance = calculate_distance(input_poi, p2)\n",
    "            category_similarity = calculate_category_similarity(input_poi, p2)\n",
    "            name_similarity = calculate_name_similarity(input_poi, p2)  # 使用修改后的函数\n",
    "            comment_similarity = calculate_comment_similarity(input_poi, p2)\n",
    "            # 添加排序优先级，默认为0，因为我们在此阶段没有实际的排序优先级\n",
    "            sort_priority = 0\n",
    "            # 构建包含所有特征的输入张量\n",
    "            input_tensor = torch.tensor([distance, category_similarity, name_similarity, comment_similarity, sort_priority], dtype=torch.float32)\n",
    "            # 使用模型预测相似度\n",
    "            output = model(input_tensor)\n",
    "            # 直接使用模型的输出作为相似度分数，注意模型输出应该是一个标量\n",
    "            similarity_score = output.item()\n",
    "            similarity_scores.append((row['id'], distance, category_similarity, name_similarity, comment_similarity, similarity_score))\n",
    "    \n",
    "    # 根据相似度分数降序排序\n",
    "    similarity_scores.sort(key=lambda x: x[5], reverse=True)\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "# 示例：使用模型获取shurupoi1.csv中poi_id为23的poi与其他poi的相似度\n",
    "poi_id = 11\n",
    "similar_pois = get_similar_pois_within_shurupoi1(model, poi_id)\n",
    "\n",
    "for poi in similar_pois[:15]:  # 注意这里我改回了只打印前5个最相似的POI\n",
    "# for poi in similar_pois[:10]:  # 打印前10个最相似的POI\n",
    "    print(f\"Poi ID: {poi[0]}, Distance: {poi[1]:.2f}, Category Similarity: {poi[2]:.2f}, Name Similarity: {poi[3]:.2f}, Comment Similarity: {poi[4]:.2f}, Final Score: {poi[5]:.2f}\")\n",
    "\n",
    "# 获取与poi_id=23最相似的前10个POI的ID\n",
    "top_poi_ids = [poi[0] for poi in similar_pois[:10]]\n",
    "\n",
    "# 读取这些POI的.csv文件，提取第二列，不再去除重复和近义词\n",
    "aspect_sets = []\n",
    "\n",
    "for id in top_poi_ids:\n",
    "    filepath = f\"ASTE5/{id}.csv\"\n",
    "    df = pd.read_csv(filepath, dtype=str)  # 明确指定所有数据为字符串类型\n",
    "    # 过滤掉空的方面词，确保只保留非空字符串\n",
    "    aspects = df.iloc[:, 1].dropna().tolist()\n",
    "    aspect_sets.append(aspects)\n",
    "\n",
    "# 读取poi_id=23的.csv文件，提取第二列，也不再去除重复和近义词\n",
    "target_filepath = f\"ASTE5/{poi_id}.csv\"\n",
    "target_df = pd.read_csv(target_filepath, dtype=str)  # 明确指定所有数据为字符串类型\n",
    "# 过滤掉空的方面词\n",
    "aspects = target_df.iloc[:, 1].dropna().tolist()\n",
    "target_aspects = aspects\n",
    "\n",
    "# 计算每个POI与目标POI的SIP值\n",
    "sips = []\n",
    "for i, aspect_set in enumerate(aspect_sets):\n",
    "    intersection = len(set(aspect_set) & set(target_aspects))\n",
    "    union = len(set(aspect_set) | set(target_aspects))\n",
    "    sip = intersection / union if union > 0 else 0\n",
    "    sips.append((top_poi_ids[i], sip))\n",
    "\n",
    "# 按照SIP值排序\n",
    "sips.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 输出SIP值\n",
    "for id, sip in sips:\n",
    "    print(f\"ID: {id}, SIP: {sip}\")\n",
    "\n",
    "# 使用KNN方法计算与poi_id=23的POI最相似的k个POI的SIP值\n",
    "# 首先按SIP值降序排序sips\n",
    "sips.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 存储K值与对应的SIP值\n",
    "k_sips = []\n",
    "\n",
    "# 遍历k值从1到10\n",
    "for k in range(1, 11):\n",
    "    # 当前的方面词集合初始化为空\n",
    "    current_aspect_set = []\n",
    "    \n",
    "    # 将当前SIP值最高的前k个POI的方面词加入到当前集合\n",
    "    for i in range(k):\n",
    "        top_poi_id, _ = sips[i]\n",
    "        index = top_poi_ids.index(top_poi_id)\n",
    "        current_aspect_set.extend(aspect_sets[index])\n",
    "    \n",
    "    # 计算SIP(pi,P) = |A_i∩A|/|A_i∪A|\n",
    "    intersection = len(set(current_aspect_set) & set(target_aspects))\n",
    "    union = len(set(current_aspect_set) | set(target_aspects))\n",
    "    sip = intersection / union if union > 0 else 0\n",
    "    k_sips.append((k, sip))\n",
    "\n",
    "# 寻找SIP值最大的K\n",
    "max_k, max_sip = max(k_sips, key=lambda x: x[1])\n",
    "\n",
    "# 输出SIP值最高的K值及对应的POI ID\n",
    "print(f\"The highest SIP value is {max_sip} at K={max_k}\")\n",
    "print(f\"The POI IDs corresponding to K={max_k} are:\")\n",
    "for i in range(max_k):\n",
    "    top_poi_id, _ = sips[i]\n",
    "    print(top_poi_id)\n",
    "\n",
    "# 绘制k取值从1到10时的SIP变化曲线\n",
    "k_values, sip_values = zip(*k_sips)\n",
    "plt.plot(k_values, sip_values)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"SIP\")\n",
    "plt.title(f\"SIP Changes with K (Ordered by SIP) for POI ID: {poi_id}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#景点类计算分数示例\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from word_similarity import WordSimilarity2010\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import xmnlp\n",
    "from xmnlp.sv import SentenceVector\n",
    "import time\n",
    "\n",
    "# 定义一个清理名称的函数，移除括号及其内容\n",
    "def clean_name(name):\n",
    "    # 使用正则表达式移除括号及其内容\n",
    "    return re.sub(r'\\(.*\\)', '', name).strip()\n",
    "\n",
    "# 修改后的名称相似度计算函数\n",
    "def calculate_name_similarity(p1, p2):\n",
    "    # 清理两个POI的名称\n",
    "    cleaned_name1 = clean_name(p1['name'])\n",
    "    cleaned_name2 = clean_name(p2['name'])\n",
    "    \n",
    "    # 现在使用清理后的名称进行比较\n",
    "    intersection = len(set(cleaned_name1) & set(cleaned_name2))\n",
    "    union = len(set(cleaned_name1) | set(cleaned_name2))\n",
    "    fiu = intersection / union if union > 0 else 0\n",
    "    return fiu\n",
    "\n",
    "# 其他代码保持不变...\n",
    "# 读取shurupoi1.csv文件\n",
    "shurupoi1 = pd.read_csv('shurupoi5.csv')\n",
    "\n",
    "# 定义获取指定ID的POI与shurupoi1.csv中其他POI的相似度的函数\n",
    "def get_similar_pois_within_shurupoi1(model, poi_id):\n",
    "    # 获取输入poi的信息\n",
    "    input_poi = shurupoi1.loc[shurupoi1['id'] == poi_id].iloc[0]\n",
    "    \n",
    "    # 初始化相似度分数列表\n",
    "    similarity_scores = []\n",
    "    \n",
    "    # 遍历shurupoi1.csv中的所有poi，计算相似度分数\n",
    "    for idx, row in shurupoi1.iterrows():\n",
    "        if row['id'] != poi_id:  # 确保不计算与自身之间的相似度\n",
    "            p2 = row\n",
    "            distance = calculate_distance(input_poi, p2)\n",
    "            category_similarity = calculate_category_similarity(input_poi, p2)\n",
    "            name_similarity = calculate_name_similarity(input_poi, p2)  # 使用修改后的函数\n",
    "            comment_similarity = calculate_comment_similarity(input_poi, p2)\n",
    "            # 添加排序优先级，默认为0，因为我们在此阶段没有实际的排序优先级\n",
    "            sort_priority = 0\n",
    "            # 构建包含所有特征的输入张量\n",
    "            input_tensor = torch.tensor([distance, category_similarity, name_similarity, comment_similarity, sort_priority], dtype=torch.float32)\n",
    "            # 使用模型预测相似度\n",
    "            output = model(input_tensor)\n",
    "            # 直接使用模型的输出作为相似度分数，注意模型输出应该是一个标量\n",
    "            similarity_score = output.item()\n",
    "            similarity_scores.append((row['id'], distance, category_similarity, name_similarity, comment_similarity, similarity_score))\n",
    "    \n",
    "    # 根据相似度分数降序排序\n",
    "    similarity_scores.sort(key=lambda x: x[5], reverse=True)\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "# 用于情感分析的函数\n",
    "def sentiment_analysis(text):\n",
    "    # 使用xmnlp进行情感分析\n",
    "    return xmnlp.sentiment(text)\n",
    "\n",
    "# 计算调整后的相似度的函数\n",
    "def adjusted_similarity(sv, query, doc):\n",
    "    # 计算原始相似度\n",
    "    raw_similarities = sv.similarity(query, doc)\n",
    "    \n",
    "    # 检查是否是单一数值\n",
    "    if isinstance(raw_similarities, np.ndarray) and raw_similarities.size > 1:\n",
    "        # 如果是数组，我们取最大相似度进行判断\n",
    "        max_similarity = np.max(raw_similarities)\n",
    "    else:\n",
    "        max_similarity = raw_similarities\n",
    "    \n",
    "    # 只有当最大相似度超过一定阈值时才进行情感分析\n",
    "    if max_similarity > 0.6:\n",
    "        # 进行情感分析\n",
    "        query_sentiment = sentiment_analysis(query)\n",
    "        doc_sentiment = sentiment_analysis(doc)\n",
    "        \n",
    "        # 情感分析输出格式为(负向概率, 正向概率)\n",
    "        query_neg_prob = query_sentiment[0]\n",
    "        doc_neg_prob = doc_sentiment[0]\n",
    "        \n",
    "        # 判断情感是否相反\n",
    "        if (query_neg_prob > 0.5 and doc_neg_prob <= 0.5) or (query_neg_prob <= 0.5 and doc_neg_prob > 0.5):\n",
    "            # 减少相似度\n",
    "            return max_similarity - 0.6\n",
    "        else:\n",
    "            return max_similarity\n",
    "    else:\n",
    "        return max_similarity\n",
    "\n",
    "# 创建SentenceVector实例\n",
    "sv = SentenceVector(genre='通用')\n",
    "\n",
    "# 定义一个函数来找出近义词（代表性）\n",
    "def find_synonyms(word, other_words, threshold_sv=0.75):\n",
    "    synonyms = []\n",
    "    for other_word in other_words:\n",
    "        sv_sim = sv.similarity(word, other_word)\n",
    "        if np.any(sv_sim > threshold_sv):\n",
    "            synonyms.append(other_word)\n",
    "    return synonyms\n",
    "\n",
    "# 定义一个函数来找出近义词（独特性）\n",
    "def find_synonymsU(word, other_words, threshold_sv=0.75):\n",
    "    synonyms = []\n",
    "    for other_word in other_words:\n",
    "        # 使用adjusted_similarity函数来计算调整后的相似度\n",
    "        sv_sim = adjusted_similarity(sv, word, other_word)\n",
    "        if np.any(sv_sim > threshold_sv):\n",
    "            synonyms.append(other_word)\n",
    "    return synonyms\n",
    "\n",
    "# 定义目录路径\n",
    "text_dir = 'chulipoi5'\n",
    "csv_dir = 'ASTE5'\n",
    "\n",
    "# 加载特定的文本文件\n",
    "def load_target_text_file(filename):\n",
    "    filepath = os.path.join(text_dir, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File {filepath} does not exist.\")\n",
    "        return [], []\n",
    "    sentences = []\n",
    "    topics = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            sentence, topic = line.strip().split('$$$')\n",
    "            sentences.append(sentence.strip())\n",
    "            topics.append(int(topic.split('##')[-1]))\n",
    "    return sentences, topics\n",
    "\n",
    "# 加载特定的CSV文件\n",
    "def load_target_csv_file(filename):\n",
    "    filepath = os.path.join(csv_dir, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File {filepath} does not exist.\")\n",
    "        return None\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 定义一个函数来处理单个POI ID\n",
    "def process_poi(poi_id):\n",
    "    # 读取shurupoi1.csv文件\n",
    "    global shurupoi1\n",
    "    if shurupoi1 is None:\n",
    "        shurupoi1 = pd.read_csv('shurupoi5.csv')\n",
    "\n",
    "    # 示例：使用模型获取shurupoi1.csv中poi_id为23的poi与其他poi的相似度\n",
    "    similar_pois = get_similar_pois_within_shurupoi1(model, poi_id)\n",
    "    \n",
    "    for poi in similar_pois[:10]:  # 注意这里我改回了只打印前5个最相似的POI\n",
    "    # for poi in similar_pois[:10]:  # 打印前10个最相似的POI\n",
    "        print(f\"Poi ID: {poi[0]}, Distance: {poi[1]:.2f}, Category Similarity: {poi[2]:.2f}, Name Similarity: {poi[3]:.2f}, Comment Similarity: {poi[4]:.2f}, Final Score: {poi[5]:.2f}\")\n",
    "\n",
    "    # 获取与poi_id最相似的前10个POI的ID\n",
    "    top_poi_ids = [poi[0] for poi in similar_pois[:10]]\n",
    "\n",
    "    # 读取这些POI的.csv文件，提取第二列，去除重复和近义词\n",
    "    ws_tool = WordSimilarity2010()\n",
    "    aspect_sets = []\n",
    "\n",
    "    for id in top_poi_ids:\n",
    "        filepath = f\"ASTE5/{id}.csv\"\n",
    "        df = pd.read_csv(filepath, dtype=str)  # 明确指定所有数据为字符串类型\n",
    "        # 过滤掉空的方面词，确保只保留非空字符串\n",
    "        aspects = df.iloc[:, 1].dropna().tolist()\n",
    "        \n",
    "        # 去重\n",
    "        unique_aspects = set(aspects)\n",
    "        \n",
    "        # 去除近义词\n",
    "        final_aspects = []\n",
    "        for aspect in unique_aspects:\n",
    "            if not any(ws_tool.similarity(aspect, other) > 0.65 for other in final_aspects):\n",
    "                final_aspects.append(aspect)\n",
    "                \n",
    "        aspect_sets.append(set(final_aspects))\n",
    "\n",
    "    # 读取poi_id的.csv文件，提取第二列，去除重复和近义词\n",
    "    target_filepath = f\"ASTE5/{poi_id}.csv\"\n",
    "    target_df = pd.read_csv(target_filepath, dtype=str)  # 明确指定所有数据为字符串类型\n",
    "    # 过滤掉空的方面词\n",
    "    aspects = target_df.iloc[:, 1].dropna().tolist()\n",
    "\n",
    "    # 去重\n",
    "    unique_aspects = set(aspects)\n",
    "\n",
    "    # 去除近义词\n",
    "    final_aspects = []\n",
    "    for aspect in unique_aspects:\n",
    "        if not any(ws_tool.similarity(aspect, other) > 0.65 for other in final_aspects):\n",
    "            final_aspects.append(aspect)\n",
    "            \n",
    "    target_aspects = set(final_aspects)\n",
    "\n",
    "    # 计算每个POI与目标POI的SIP值\n",
    "    sips = []\n",
    "    for i, aspect_set in enumerate(aspect_sets):\n",
    "        intersection = len(aspect_set & target_aspects)\n",
    "        union = len(aspect_set | target_aspects)\n",
    "        sip = intersection / union if union > 0 else 0\n",
    "        sips.append((top_poi_ids[i], sip))\n",
    "\n",
    "    # 按照SIP值排序\n",
    "    sips.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 输出SIP值前4的POI ID和SIP分数\n",
    "    top_sip_pois = []\n",
    "    print(\"Top 2 POI IDs and their SIP scores:\")\n",
    "    for i in range(2):\n",
    "        id, sip = sips[i]\n",
    "        top_sip_pois.append(id)\n",
    "        print(f\"POI ID: {id}, SIP score: {sip:.2f}\")\n",
    "    \n",
    "    # 保存SIP排名前4的POI ID到变量中\n",
    "    top_sip_ids = top_sip_pois\n",
    "\n",
    "    # 加载目标文本文件\n",
    "    target_txt_filename = f\"{poi_id}.txt\"\n",
    "    sentences, topics = load_target_text_file(target_txt_filename)\n",
    "    \n",
    "    # 加载目标CSV文件\n",
    "    target_csv_filename = f\"{poi_id}.csv\"\n",
    "    df = load_target_csv_file(target_csv_filename)\n",
    "    \n",
    "    # 检查是否成功加载了CSV文件\n",
    "    if df is None:\n",
    "        raise ValueError(f\"Failed to load the CSV file for POI ID {poi_id}.\")\n",
    "\n",
    "    # 将DataFrame添加到字典中，键为CSV文件名\n",
    "    dfs = {target_csv_filename: df}\n",
    "    \n",
    "    # 将句子和主题序号关联起来\n",
    "    sentences_topics = list(zip(sentences, topics))\n",
    "    \n",
    "    # 计算每个主题的出现次数\n",
    "    topic_counts = pd.Series(topics).value_counts()\n",
    "    print(\"Topic counts:\", topic_counts)\n",
    "    # 选择出现次数最多的前五个主题\n",
    "    top_topics = topic_counts.head(5).index.tolist()\n",
    "    print(\"Top 5 topics:\", top_topics)\n",
    "    \n",
    "    # 过滤掉非顶级主题的句子\n",
    "    filtered_sentences_topics = [(sentence, topic) for sentence, topic in sentences_topics if topic in top_topics]\n",
    "    \n",
    "    # 计算代表性分数的函数\n",
    "    def calculate_representativeness(sentences_topics, df):\n",
    "        # 初始化一个字典来存储每个句子的代表性分数\n",
    "        representativeness_scores = {}\n",
    "    \n",
    "        # 删除方面词为空的行\n",
    "        df = df[~df['a'].isna() & (df['a'].str.strip() != '')]\n",
    "    \n",
    "        # 对于每个句子，找到对应的行，并计算代表性分数\n",
    "        for index, (sentence, topic) in enumerate(sentences_topics):\n",
    "            sentence_clean = sentence.split('$$$')[0].strip()  # 假设'$$$'是分割符\n",
    "        \n",
    "            # 句子在.txt文件中的行号（也是.csv文件中的id）\n",
    "            row_num = index + 1\n",
    "        \n",
    "            # 在CSV文件中找到与句子行号对应的行，并且只考虑与当前句子相同主题的行\n",
    "            related_rows = df[(df['id'] == row_num) & (df['ZT'] == topic)]\n",
    "        \n",
    "            # 初始化该句子的代表性分数为0\n",
    "            rep_score = 0\n",
    "        \n",
    "            for _, row in related_rows.iterrows():\n",
    "                # 获取该行的方面词和意见词\n",
    "                aspect = row['a']\n",
    "                opinion = row['o']\n",
    "                \n",
    "                # 只考虑与当前句子相同主题的方面词和意见词\n",
    "                same_topic_rows = df[df['ZT'] == topic]\n",
    "            \n",
    "                # 找出意见词的近义词\n",
    "                opinion_synonyms = find_synonyms(opinion, same_topic_rows['o'].unique(), threshold_sv=0.7)\n",
    "            \n",
    "                # 找出方面词的近义词\n",
    "                aspect_synonyms = find_synonyms(aspect, same_topic_rows['a'].unique(), threshold_sv=0.7)\n",
    "            \n",
    "                # 初始化P(o|a)和P(o|a̅)\n",
    "                p_o_given_a = 0\n",
    "                p_o_given_not_a = 0\n",
    "                total_synonyms_count = len(opinion_synonyms)\n",
    "            \n",
    "                # 对于每个近义词计算P(o|a)和P(o|a̅)，使用整个DataFrame (df) 来计算\n",
    "                for synonym in opinion_synonyms:\n",
    "                    # 计算P(o|a)：意见词在当前方面词及其近义词中的计数\n",
    "                    count_o_given_a = len(same_topic_rows[(same_topic_rows['o'] == synonym) & (same_topic_rows['a'].isin(aspect_synonyms))])\n",
    "                    p_o_given_a += count_o_given_a\n",
    "                \n",
    "                    # 计算P(o|a̅)：意见词不在当前方面词及其近义词中的计数\n",
    "                    count_o_given_not_a = len(same_topic_rows[(same_topic_rows['o'] == synonym) & (~same_topic_rows['a'].isin(aspect_synonyms))])\n",
    "                    p_o_given_not_a += count_o_given_not_a\n",
    "            \n",
    "                # 根据公式计算代表性分数\n",
    "                rep = p_o_given_a / (0.5 + p_o_given_not_a)\n",
    "                \n",
    "                # 将代表性分数累加到该句子的总分中\n",
    "                rep_score += rep\n",
    "        \n",
    "            # 将最终的代表性分数添加到句子的总分中\n",
    "            representativeness_scores[sentence_clean] = rep_score\n",
    "            \n",
    "            # 输出最终的代表性分数\n",
    "            print(f\"Final REP Score for Sentence {index + 1}: {representativeness_scores[sentence_clean]}\")\n",
    "    \n",
    "        # 找到所有句子的代表性分数中的最大值\n",
    "        max_rep_score = max(representativeness_scores.values())\n",
    "    \n",
    "        # 归一化代表性分数\n",
    "        normalized_representativeness_scores = {sentence: score / max_rep_score for sentence, score in representativeness_scores.items()}\n",
    "                \n",
    "        return normalized_representativeness_scores\n",
    "    \n",
    "    # 计算句子的SS值\n",
    "    def calculate_ss(sentences_topics):\n",
    "        ss_scores = {}\n",
    "\n",
    "        for index, (sentence, topic) in enumerate(sentences_topics):\n",
    "            sentence_clean = sentence.split('$$$')[0].strip()  # 假设'$$$'是分割符\n",
    "            similarity_counts = 0\n",
    "            \n",
    "            # 只考虑与当前句子相同主题的其他句子\n",
    "            same_topic_sentences = [s for s, t in sentences_topics if t == topic]\n",
    "        \n",
    "            # 包括句子本身在内的相同主题的句子数量\n",
    "            total_same_topic_sentences = len(same_topic_sentences)\n",
    "        \n",
    "            # 计算句子与其他相同主题的句子之间的相似度\n",
    "            for other_sentence, _ in sentences_topics:\n",
    "                if other_sentence in same_topic_sentences:\n",
    "                    adj_similarity = adjusted_similarity(sv, sentence, other_sentence)\n",
    "                \n",
    "                    if adj_similarity > 0.6:\n",
    "                        similarity_counts += 1\n",
    "        \n",
    "            # 计算SS值\n",
    "            ss_value = similarity_counts / total_same_topic_sentences\n",
    "        \n",
    "            # 输出最终的SS值\n",
    "            print(f\"Final SS Value for Sentence {index + 1}: {ss_value}\")\n",
    "        \n",
    "            ss_scores[sentence_clean] = ss_value\n",
    "\n",
    "        return ss_scores\n",
    "\n",
    "    # 计算独特性分数的函数\n",
    "    def calculate_uniqueness(sentences_topics, dfs, top_sip_ids):\n",
    "        uniqueness_scores = {}\n",
    "        ss_scores = calculate_ss(sentences_topics)  # 先计算SS值\n",
    "    \n",
    "        # 合并所有top_sip_ids对应的CSV文件\n",
    "        combined_df = pd.concat([load_target_csv_file(f\"{top_id}.csv\") for top_id in top_sip_ids], ignore_index=True)\n",
    "    \n",
    "        # 删除方面词为空的行\n",
    "        combined_df = combined_df[~combined_df['a'].isna() & (combined_df['a'].str.strip() != '')]\n",
    "    \n",
    "        # 遍历每个句子\n",
    "        for index, (sentence, topic) in enumerate(sentences_topics):\n",
    "            sentence_clean = sentence.split('$$$')[0].strip()  # 假设'$$$'是分割符\n",
    "        \n",
    "            # 句子在.txt文件中的行号（也是.csv文件中的id）\n",
    "            row_num = index + 1\n",
    "        \n",
    "            # 在CSV文件中找到与句子行号对应的行\n",
    "            df = dfs[next(iter(dfs))]  # 获取字典中的第一个（也是唯一一个）CSV文件\n",
    "            related_rows = df[(df['id'] == row_num) & (df['ZT'] == topic)]\n",
    "        \n",
    "            # 初始化该句子的独特性分数为0\n",
    "            uniq_score = 0\n",
    "        \n",
    "            # 对于每一组数据来进行2-4的操作\n",
    "            SAO_sum = 0  # 用于累积SAO的值\n",
    "            for _, row in related_rows.iterrows():\n",
    "                # 获取该组数据中的方面词和意见词\n",
    "                aspect = row['a']\n",
    "                opinion = row['o']\n",
    "            \n",
    "                # 查找与方面词相同或近义的词，只考虑与当前句子相同主题的行\n",
    "                aspect_synonyms = find_synonymsU(aspect, combined_df[combined_df['ZT'] == topic]['a'].unique(), threshold_sv=0.7)\n",
    "                \n",
    "            \n",
    "                # 计算SS(a): 近义方面词的出现次数\n",
    "                SS_a = len(combined_df[(combined_df['a'].isin(aspect_synonyms)) & (combined_df['ZT'] == topic)])\n",
    "                \n",
    "            \n",
    "                # 对于每个找到的方面词，计算其对应的意见词在大集合中相同或近义的出现次数\n",
    "                SS_o = 0\n",
    "                for syn_aspect in aspect_synonyms:\n",
    "                    opinion_synonyms = find_synonymsU(opinion, combined_df[(combined_df['a'] == syn_aspect) & (combined_df['ZT'] == topic)]['o'].unique(), threshold_sv=0.7)\n",
    "                    \n",
    "                    SS_o += len(combined_df[(combined_df['a'] == syn_aspect) & (combined_df['o'].isin(opinion_synonyms)) & (combined_df['ZT'] == topic)])\n",
    "                    \n",
    "            \n",
    "                # 如果SS(a)为0，则将SS(o)/SS(a)定义为0\n",
    "                SAO = SS_o / SS_a if SS_a != 0 else 0\n",
    "                \n",
    "            \n",
    "                # 累积SAO\n",
    "                SAO_sum += SAO\n",
    "        \n",
    "            # 计算独特性分数\n",
    "            uniq = ss_scores[sentence_clean] * (1 / (1 + np.log10(1 + SAO_sum)))\n",
    "              \n",
    "            # 输出每个句子的独特性分数\n",
    "            print(f\"Final Uniq Score for Sentence {index + 1}: {uniq}\")\n",
    "        \n",
    "            # 更新独特性分数字典\n",
    "            uniqueness_scores[sentence_clean] = {\n",
    "                'UNQ': uniq,\n",
    "                'SS': ss_scores[sentence_clean]\n",
    "            }\n",
    "        \n",
    "        # 找到所有句子的独特性分数中的最大值\n",
    "        max_uniq_score = max(score['UNQ'] for score in uniqueness_scores.values())\n",
    "\n",
    "        # 归一化独特性分数\n",
    "        normalized_uniqueness_scores = {sentence: {'UNQ': score['UNQ'] / max_uniq_score, 'SS': score['SS']} for sentence, score in uniqueness_scores.items()}\n",
    "\n",
    "        return normalized_uniqueness_scores\n",
    "\n",
    "    # 计算多样性分数的函数\n",
    "    def calculate_diversity(sentences, dfs):\n",
    "        diversity_scores = []\n",
    "        # 从dfs字典中获取对应的CSV文件\n",
    "        df = next(iter(dfs.values()))  # 获取字典中的第一个（也是唯一一个）CSV文件\n",
    "    \n",
    "        # 计算.csv文件中出现次数最多的id的出现次数\n",
    "        max_count = df['id'].value_counts().max()\n",
    "    \n",
    "        for index, (sentence, topic) in enumerate(sentences_topics):\n",
    "            sentence_clean = sentence.split('$$$')[0].strip()  # 假设'$$$'是分割符\n",
    "        \n",
    "            # 句子在.txt文件中的行号（也是.csv文件中的id）\n",
    "            row_num = index + 1\n",
    "        \n",
    "            # 在CSV文件中找到与句子行号对应的行，并且只考虑与当前句子相同主题的行\n",
    "            related_rows = df[(df['id'] == row_num) & (df['ZT'] == topic)]\n",
    "        \n",
    "            # 计算id出现的次数\n",
    "            count = related_rows.shape[0]\n",
    "            diversity_scores.append(count / max_count)\n",
    "            \n",
    "    \n",
    "        return diversity_scores\n",
    "\n",
    "    representativeness_scores = calculate_representativeness(filtered_sentences_topics, df)\n",
    "\n",
    "    uniqueness_scores = calculate_uniqueness(filtered_sentences_topics, dfs, top_sip_ids)\n",
    "\n",
    "    diversity_scores = calculate_diversity(filtered_sentences_topics, dfs)\n",
    "    \n",
    "    # 创建一个空字典来存储每个句子的四个评分\n",
    "    all_scores = {}\n",
    "\n",
    "    for index, (sentence, topic) in enumerate(filtered_sentences_topics):\n",
    "        # 对每个评分进行四舍五入保留四位小数\n",
    "        rep_score = round(representativeness_scores.get(sentence, 0), 4)\n",
    "        unq_score = round(uniqueness_scores.get(sentence, {'UNQ': 0})['UNQ'], 4)\n",
    "        div_score = round(diversity_scores[index], 4)\n",
    "        \n",
    "        all_scores[sentence] = {\n",
    "            'ID': index + 1,\n",
    "            'sentence': sentence,\n",
    "            'REP': rep_score,\n",
    "            'UNQ': unq_score,\n",
    "            'DIV': div_score,\n",
    "            'ZT': topic\n",
    "        }\n",
    "\n",
    "    return top_sip_ids, all_scores  # 返回必要的结果，以便可以在外部进行进一步处理\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 定义要处理的POI ID范围\n",
    "    start_poi_id = 1\n",
    "    end_poi_id = 20\n",
    "\n",
    "    # 循环处理每个POI ID\n",
    "    for poi_id in range(start_poi_id, end_poi_id + 1):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # 处理每个POI\n",
    "        top_sip_ids, all_scores = process_poi(poi_id)\n",
    "\n",
    "        # 输出或保存结果\n",
    "        # 例如，你可以保存all_scores到CSV文件\n",
    "        output_df = pd.DataFrame(all_scores).T\n",
    "        output_df.columns = ['ID', 'sentence', 'REP', 'UNQ', 'DIV', 'ZT']\n",
    "\n",
    "        # 对数据框进行排序\n",
    "        output_df.sort_values(by=['ZT', 'ID'], inplace=True)\n",
    "\n",
    "        # 保存到CSV文件\n",
    "        output_filename = os.path.join('fenshu5', f\"{poi_id}.csv\")\n",
    "        output_df.to_csv(output_filename, index=False)\n",
    "        print(f\"已完成 {poi_id}.csv\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"{poi_id}.csv took {execution_time:.4f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输出摘要示例\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xmnlp.sv import SentenceVector\n",
    "\n",
    "# 定义文件路径\n",
    "input_folder = 'fenshu5'\n",
    "aste_folder = 'ASTE5'\n",
    "output_folder = 'xin5'\n",
    "\n",
    "# 创建输出文件夹\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 定义句子相似度计算函数\n",
    "def calculate_similarity(sentence1, sentence2, sv):\n",
    "    # 先将句子转换为向量表示\n",
    "    vec1 = sv.transform(sentence1)\n",
    "    vec2 = sv.transform(sentence2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# 定义MMV计算函数\n",
    "def calculate_mmv(sentence, sentences_set, sv):\n",
    "    max_similarity = 0\n",
    "    for s in sentences_set:\n",
    "        sim = calculate_similarity(sentence, s, sv)\n",
    "        if sim > max_similarity:\n",
    "            max_similarity = sim\n",
    "    # MMV计算公式保持不变\n",
    "    mmv = 0.35 * sentence['score'] - 0.65 * max_similarity\n",
    "    return mmv, 0.35 * sentence['score'], 0.65 * max_similarity\n",
    "\n",
    "# 处理每个CSV文件\n",
    "for file_number in range(1, 21):\n",
    "    filename = f\"{file_number}.csv\"\n",
    "    print(f\"Processing {filename}\")\n",
    "\n",
    "    # 读取fenshu5文件夹中的文件\n",
    "    fenshu_path = os.path.join(input_folder, filename)\n",
    "    df_fenshu = pd.read_csv(fenshu_path)\n",
    "    print(f\"Read {df_fenshu.shape[0]} rows from {fenshu_path}\")\n",
    "\n",
    "    # 读取ASTE文件夹中的对应文件\n",
    "    aste_path = os.path.join(aste_folder, f\"{file_number}.csv\")\n",
    "    df_aste = pd.read_csv(aste_path)\n",
    "    print(f\"Read {df_aste.shape[0]} rows from {aste_path}\")\n",
    "\n",
    "    # 初始化SentenceVector对象\n",
    "    sv = SentenceVector(genre='通用')\n",
    "\n",
    "    # 计算每个句子的总体分数\n",
    "    df_fenshu['score'] = 0.35 * df_fenshu['REP'] + 0.55 * df_fenshu['UNQ'] + 0.1 * df_fenshu['DIV']\n",
    "    print(\"Calculated scores for all sentences.\")\n",
    "\n",
    "    # 计算每个主题的句子数量\n",
    "    theme_counts = df_fenshu['ZT'].value_counts().sort_values(ascending=False)\n",
    "    print(f\"Theme counts: {theme_counts}\")\n",
    "\n",
    "    # 按照主题分数排序，每个主题下的句子按总体分数排序\n",
    "    sorted_df = df_fenshu.sort_values(by=['ZT', 'score'], ascending=[True, False])\n",
    "    print(\"Sorted DataFrame by themes and scores.\")\n",
    "\n",
    "    # 如果总句子数少于5个，则直接使用这些句子\n",
    "    if sorted_df.shape[0] < 5:\n",
    "        print(\"Less than 5 sentences in the document, using all available sentences.\")\n",
    "        selected_sentences = sorted_df.copy()\n",
    "    else:\n",
    "        # 初始化待选摘要集\n",
    "        selected_sentences = []\n",
    "\n",
    "        # 选择每个主题的最佳句子\n",
    "        theme_index = 0\n",
    "        while len(selected_sentences) < 5 and theme_index < len(theme_counts.index):\n",
    "            theme = theme_counts.index[theme_index]\n",
    "            print(f\"Processing theme {theme}\")\n",
    "            theme_df = sorted_df[sorted_df['ZT'] == theme]\n",
    "\n",
    "            # 从主题中选择句子\n",
    "            if len(selected_sentences) == 0:\n",
    "                # 第一次选择，直接选择分数最高的句子\n",
    "                best_sentence = theme_df.iloc[0]\n",
    "                best_sentence['mmv'] = best_sentence['score']  # 添加mmv列并初始化为score\n",
    "                selected_sentences.append(best_sentence)\n",
    "                print(f\"Selected sentence: {best_sentence['sentence']}, score: {best_sentence['score']}\")\n",
    "            else:\n",
    "                # 计算MMV并选择最佳句子\n",
    "                mmv_scores = []\n",
    "                for rank, row in theme_df.iterrows():\n",
    "                    mmv, score_part, similarity_part = calculate_mmv(row, selected_sentences, sv)\n",
    "                    print(f\"MMV for sentence {row['sentence']}: {mmv}, score part: {score_part}, similarity part: {similarity_part}\")\n",
    "                    mmv_scores.append((row, mmv))\n",
    "\n",
    "                # 获取所有MMV值\n",
    "                mmv_values = [x[1] for x in mmv_scores]\n",
    "\n",
    "                # 找到MMV的最大绝对值\n",
    "                max_abs_mmv = max(abs(x) for x in mmv_values)\n",
    "\n",
    "                # 调整所有MMV值，使其非负\n",
    "                adjusted_mmv_scores = [(row, mmv + max_abs_mmv) for row, mmv in mmv_scores]\n",
    "\n",
    "                # 选择调整后的MMV最高的句子\n",
    "                if adjusted_mmv_scores:\n",
    "                    best_row, best_mmv = max(adjusted_mmv_scores, key=lambda x: x[1])\n",
    "                    best_row['mmv'] = best_mmv  # 添加mmv列\n",
    "                    selected_sentences.append(best_row)\n",
    "                    print(f\"Selected sentence: {best_row['sentence']}, score: {best_row['score']}\")\n",
    "\n",
    "            # 如果已经选出了5个句子，退出循环\n",
    "            if len(selected_sentences) == 5:\n",
    "                break\n",
    "\n",
    "            theme_index += 1\n",
    "\n",
    "        # 如果主题数量不足5个，重新开始选择\n",
    "        while len(selected_sentences) < 5:\n",
    "            for theme_index in range(len(theme_counts.index)):\n",
    "                theme = theme_counts.index[theme_index]\n",
    "                print(f\"Processing theme {theme} again\")\n",
    "                theme_df = sorted_df[sorted_df['ZT'] == theme]\n",
    "\n",
    "                # 从主题中选择句子\n",
    "                mmv_scores = []\n",
    "                for rank, row in theme_df.iterrows():\n",
    "                    mmv, score_part, similarity_part = calculate_mmv(row, selected_sentences, sv)\n",
    "                    print(f\"MMV for sentence {row['sentence']}: {mmv}, score part: {score_part}, similarity part: {similarity_part}\")\n",
    "                    mmv_scores.append((row, mmv))\n",
    "\n",
    "                # 获取所有MMV值\n",
    "                mmv_values = [x[1] for x in mmv_scores]\n",
    "\n",
    "                # 找到MMV的最大绝对值\n",
    "                max_abs_mmv = max(abs(x) for x in mmv_values)\n",
    "\n",
    "                # 调整所有MMV值，使其非负\n",
    "                adjusted_mmv_scores = [(row, mmv + max_abs_mmv) for row, mmv in mmv_scores]\n",
    "\n",
    "                # 选择调整后的MMV最高的句子\n",
    "                if adjusted_mmv_scores:\n",
    "                    best_row, best_mmv = max(adjusted_mmv_scores, key=lambda x: x[1])\n",
    "                    best_row['mmv'] = best_mmv  # 添加mmv列\n",
    "                    selected_sentences.append(best_row)\n",
    "                    print(f\"Selected sentence: {best_row['sentence']}, score: {best_row['score']}\")\n",
    "\n",
    "                # 如果已经选出了5个句子，退出循环\n",
    "                if len(selected_sentences) == 5:\n",
    "                    break\n",
    "\n",
    "    # 输出最终选择的句子\n",
    "    final_sentences = [row['sentence'] for row in selected_sentences[:5]]\n",
    "    print(f\"Final sentences: {final_sentences}\")\n",
    "\n",
    "    # 输出最终结果\n",
    "    output_filename = os.path.join(output_folder, f\"{file_number}.csv\")\n",
    "    pd.DataFrame({'sentence': final_sentences}).to_csv(output_filename, index=False)\n",
    "    print(f\"Output saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcc7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实验示例\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "\n",
    "# 读取数据，并记录句子和行号的映射关系\n",
    "def read_data(txt_folder, csv_folder, id):\n",
    "    sentences = []\n",
    "    sentence_to_row = {}  # 记录句子和行号的映射\n",
    "    aspects_dict = {}  # 记录id和方面词的映射\n",
    "    \n",
    "    txt_path = os.path.join(txt_folder, f'{id}.txt')\n",
    "    csv_path = os.path.join(csv_folder, f'{id}.csv')\n",
    "    \n",
    "    with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file, 1):\n",
    "            parts = line.strip().split('$$$')\n",
    "            sentence = parts[0].strip()\n",
    "            sentences.append(sentence)\n",
    "            sentence_to_row[sentence] = line_number\n",
    "\n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "    for index, row in csv_data.iterrows():\n",
    "        aspects_dict[row[\"id\"]] = {'a': row['a'], 'ZT': row['ZT'], 'f': row['f']}  # 添加f列\n",
    "\n",
    "    return sentences, sentence_to_row, aspects_dict\n",
    "\n",
    "# 根据摘要句子获取对应的方面词及情感分数\n",
    "def get_aspects_for_summary_sentences(chulipoi_data, aste_data, summary_sentences):\n",
    "    summary_aspects = []  # 存储所有摘要句子的方面词\n",
    "    sentiment_scores = []  # 存储所有摘要句子的情感分数\n",
    "    \n",
    "    for sentence in summary_sentences:\n",
    "        row = chulipoi_data[sentence]  # 根据句子获取行号\n",
    "        # 根据行号获取所有相关的方面词和情感分数\n",
    "        sentiment_score = 0.5  # 初始化情感分数\n",
    "        for index, row_data in aste_data.iterrows():\n",
    "            if str(row_data['id']) == str(row):\n",
    "                summary_aspects.append({'a': row_data['a'], 'ZT': row_data['ZT'], 'f': row_data['f']})\n",
    "                sentiment_score += 0.1 if row_data['f'] == 1 else -0.1 if row_data['f'] == 0 else 0\n",
    "                \n",
    "        sentiment_scores.append(sentiment_score)\n",
    "        \n",
    "    return summary_aspects\n",
    "\n",
    "# 计算信息覆盖率\n",
    "def calculate_coverage(text, summary_text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    text_tfidf = vectorizer.fit_transform([text])\n",
    "    summary_tfidf = vectorizer.transform([summary_text])\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(summary_tfidf, text_tfidf)\n",
    "    coverage = max(similarity_matrix[0])\n",
    "    return coverage\n",
    "\n",
    "# 计算信息多样性\n",
    "def calculate_diversity(summary_aspects):\n",
    "    # 分组方面词按主题（ZT）\n",
    "    aspect_counts_by_ZT = {}\n",
    "    for aspect in summary_aspects:\n",
    "        ZT = aspect['ZT']\n",
    "        a = aspect['a']\n",
    "        # 跳过空的方面词\n",
    "        if pd.isna(a):\n",
    "            continue\n",
    "        \n",
    "        if ZT not in aspect_counts_by_ZT:\n",
    "            aspect_counts_by_ZT[ZT] = {}\n",
    "        if a not in aspect_counts_by_ZT[ZT]:\n",
    "            aspect_counts_by_ZT[ZT][a] = 0\n",
    "        aspect_counts_by_ZT[ZT][a] += 1\n",
    "\n",
    "\n",
    "    # 获取每个主题下所有方面词的总数\n",
    "    all_aspect_counts_by_ZT = {}\n",
    "    for ZT, aspects in aspect_counts_by_ZT.items():\n",
    "        total_aspects = sum(aspects.values())\n",
    "        all_aspect_counts_by_ZT[ZT] = total_aspects\n",
    "\n",
    "\n",
    "    total_diversity = 0.0\n",
    "    for ZT, aspects in aspect_counts_by_ZT.items():\n",
    "        total_aspects = all_aspect_counts_by_ZT[ZT]\n",
    "        if total_aspects > 0:\n",
    "            # 计算每个方面词出现的概率\n",
    "            probabilities = [count / total_aspects for count in aspects.values()]\n",
    "\n",
    "            # 使用信息熵公式计算多样性\n",
    "            diversity = -sum([(count + 1) / total_aspects * math.log((count + 0.1) / total_aspects) for count in aspects.values()])\n",
    "            total_diversity += diversity * 0.2\n",
    "    return total_diversity\n",
    "\n",
    "# 主程序\n",
    "txt_folder = 'chulipoi5'\n",
    "csv_folder = 'ASTE5'\n",
    "summary_folder = 'zhaiyao5'\n",
    "\n",
    "# 初始化累加器\n",
    "total_coverage = 0.0\n",
    "total_diversity = 0.0\n",
    "num_files = 0\n",
    "\n",
    "for i in range(1, 21):\n",
    "    # 读取数据\n",
    "    sentences, sentence_to_row, aspects_dict = read_data(txt_folder, csv_folder, i)\n",
    "\n",
    "    # 将所有句子连接成一个文本\n",
    "    text = \" \".join(sentences)\n",
    "    \n",
    "    # 读取摘要句子\n",
    "    summary_csv_path = os.path.join(summary_folder, f'{i}.csv')\n",
    "    summary_csv_data = pd.read_csv(summary_csv_path)\n",
    "    summary_sentences = summary_csv_data['sentence'].tolist()\n",
    "\n",
    "    # 将摘要句子连接成一个文本\n",
    "    summary_text = \" \".join(summary_sentences)\n",
    "\n",
    "    # 计算信息覆盖率\n",
    "    coverage = calculate_coverage(text, summary_text)\n",
    "    total_coverage += coverage\n",
    "    \n",
    "\n",
    "    # 计算信息多样性\n",
    "    diversity = calculate_diversity(summary_aspects)\n",
    "    total_diversity += diversity\n",
    "\n",
    "    num_files += 1\n",
    "\n",
    "# 计算平均值\n",
    "average_coverage = total_coverage / num_files\n",
    "average_diversity = total_diversity / num_files\n",
    "\n",
    "print(f'所有文件的平均信息覆盖率: {average_coverage}')\n",
    "print(f'所有文件的平均信息多样性: {average_diversity}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
